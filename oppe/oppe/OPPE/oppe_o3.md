Great. I’ll put together three Python files implementing Off-policy Policy Evaluation (OPE) methods—Importance Sampling, Direct Method, and Doubly Robust—from scratch, using RLlib 2.11 PPO policies for action selection, and input JSON episodes from the LunarLander-v2 environment. I’ll also include detailed theoretical explanations of each method.

I’ll let you know as soon as everything is ready for your review.


# Off-Policy Policy Evaluation (OPE) for LunarLander-v2

Off-policy policy evaluation (OPE) is the problem of estimating the expected value (e.g. total reward) of a **target policy** using data collected from a different **behavior policy**. This is crucial in scenarios where running the target policy in the real environment is expensive or risky. For example, we might have an agent trained via PPO (the target policy) and a batch of episodes generated by another policy (the behavior policy), and we want to estimate how well the PPO agent would perform **without** executing it on the actual environment. In this answer, we explain three fundamental OPE methods—**Importance Sampling (IS)**, **Direct Method (DM)**, and **Doubly Robust (DR)**—and provide from-scratch Python implementations for each. We will focus on the **LunarLander-v2** environment (Gymnasium version), which has a continuous state (8-dimensional) and a discrete action space of 4 actions (0–3). We assume the offline data is provided as a JSON containing episodes (each episode is a list of transitions with fields like `state, action, reward, next_state, behavior_policy_probs`). The target policy is a PPO policy (from RLlib 2.11) which we can query for action probabilities. For OPE to be feasible, we assume **coverage**: the behavior policy must have non-zero probability for any action that the target policy might take in a given state (π(a|s) > 0 ⇒ μ(a|s) > 0), so that importance weights remain defined.

**Summary of Approaches:** In general, OPE methods trade off bias and variance. **Direct methods** rely on modeling the environment or value function from data, yielding low variance but potential bias if the model is misspecified. **Importance sampling** methods use the **importance weight** (the probability ratio between target and behavior policies) to re-weight observed rewards, yielding an **unbiased** estimator independent of model assumptions, but often high variance when the target and behavior policies differ significantly. **Doubly robust** methods combine these to get the best of both: they remain unbiased as long as *either* the model is accurate or the importance weights are correct, and typically have lower variance than plain IS. Below, we explain each method in detail and provide code implementations.

## Importance Sampling (IS) Method

**Theory:** Importance Sampling OPE uses the idea of weighting trajectory returns by the likelihood ratio of the target policy to the behavior policy. Intuitively, we **upweight** trajectory outcomes that the target policy would produce more often than the behavior did, and downweight those it would produce less often. For a single episode, the **importance weight** is:

$\rho_{0:T} \;=\; \prod_{t=0}^{T-1} \frac{\pi(a_t \mid s_t)}{\mu(a_t \mid s_t)} \,,$

where $\mu(a_t|s_t)$ is the behavior policy’s probability of taking action $a_t$ in state $s_t$, and $\pi(a_t|s_t)$ is the target policy’s probability for the same. This ratio $\rho\_{0\:T}$ represents how much more (or less) probable the entire trajectory is under the target policy compared to the behavior policy. Given an episode return $G$ (the sum of rewards in the episode), an **ordinary importance sampling** estimator of the target policy’s value is the average of $\rho \cdot G$ over many episodes:

$V_{\text{IS}}(\pi) \;\approx\; \frac{1}{N} \sum_{i=1}^{N} \rho^{(i)} \, G^{(i)} \,,$

where $\rho^{(i)}$ is the product of ratios for episode $i$ and $G^{(i)}$ is the total reward of that episode following behavior $\mu$. This estimator is **unbiased** (assuming correct knowledge of $\mu$ ), meaning $E[V\_{\text{IS}}] = V(\pi)$, but it can suffer extremely high variance for long horizons or if $\pi$ diverges from $\mu$. A common variant is **weighted IS**, which normalizes the weights (dividing by $\sum \rho$) to reduce variance at the cost of introducing bias. In practice, weighted IS often dramatically lowers variance and is preferred when some bias is acceptable.

**Key points of IS:**

* *Unbiasedness:* If the behavior policy probabilities $\mu(a|s)$ are known (or accurately estimated), IS yields an unbiased estimate of the target policy’s value.
* *High variance:* Large importance weights (e.g. when $\pi$ chooses actions that $\mu$ rarely did) can make the estimate noisy. Variance grows exponentially with episode length in the worst case.
* *Requirement:* The support of $\pi$ must be contained in the support of $\mu$ (coverage), otherwise $\pi(a|s)/\mu(a|s)$ can be undefined or huge. Ensure the behavior policy at least occasionally took the actions that the target policy would take in those states.
* *Improvements:* Techniques like **per-decision IS** (applying IS on a per-step basis and accumulating partial returns) and weighted IS can mitigate variance. Doubly robust (discussed later) further reduces variance by using a value function approximation as a baseline.

**Implementation:** Below is a Python implementation of off-policy evaluation with ordinary importance sampling. It assumes we have the data as a list of episodes (`episodes`), where each episode is a list of step dicts containing at least `state`, `action`, `reward`, and `behavior_policy_prob`(s). We also assume we have access to the target policy’s probability $\pi(a|s)$, for example via a function or object `eval_policy` with a method `prob(state, action)`. In an RLlib context, you might obtain `eval_policy` by loading the PPO policy and querying its action distribution for each state (e.g. using `policy.compute_single_action(..., full_fetch=True)` to get probabilities). The code multiplies importance ratios across each episode and averages the weighted returns:

```python
# importance_sampling_ope.py
import json

def off_policy_eval_importance_sampling(episodes, eval_policy):
    """
    Off-Policy Evaluation using Ordinary Importance Sampling.
    - episodes: list of episodes, each a list of steps (dicts with keys: state, action, reward, etc.).
    - eval_policy: object with eval_policy.prob(state, action) -> π(a|s), the target policy's probability for (state, action).
    Returns: Estimated value (expected return) of the target policy.
    """
    total_weighted_return = 0.0
    n_episodes = len(episodes)
    for ep in episodes:
        # Initialize importance weight for this episode
        rho = 1.0
        G = 0.0  # cumulative reward for this episode
        for step in ep:
            s = step["state"]
            a = step["action"]
            r = step["reward"]
            G += r  # accumulate reward (assuming episode return is sum of rewards; use discounting here if needed)
            # Probability of this action under behavior policy:
            if "behavior_policy_prob" in step:
                mu_prob = step["behavior_policy_prob"]  # already given for the taken action
            else:
                # If behavior_policy_probs is a list of probs for all actions, use the taken action's prob
                mu_prob = step["behavior_policy_probs"][a]
            # Probability of this action under target policy:
            pi_prob = eval_policy.prob(s, a)
            # Update cumulative importance weight
            if mu_prob == 0:
                # If behavior policy never took this action (mu_prob=0), skip or set weight to 0 (no coverage)
                rho = 0.0
                break
            rho *= pi_prob / mu_prob
        # Add weighted return for this episode
        total_weighted_return += rho * G
    # Ordinary IS estimator: average weighted returns
    return total_weighted_return / n_episodes

# Example usage:
# episodes_data = json.load(open("episodes.json"))
# value_estimate = off_policy_eval_importance_sampling(episodes_data, eval_policy)
# print("Estimated policy value (IS):", value_estimate)
```

This function computes the ordinary IS estimate. It loops over each episode, accumulates the total reward `G` and the product of probability ratios `rho`. Finally, it averages $\rho \cdot G$ over all episodes. If a behavior probability `mu_prob` is zero for a needed action (no coverage), the code handles it by breaking out (effectively assigning weight 0 to that episode’s return). In practice, one might simply skip or ignore episodes where coverage fails, but such episodes indicate the target policy is taking an action never seen in data, making the OPE estimate unreliable for that part of state space.

## Direct Method (DM)

**Theory:** The Direct Method for OPE (sometimes called **model-based OPE** or **value function regression**) involves **learning a model of the target policy’s value (or Q-function)** from the offline data, and then using that model to directly predict the target policy’s performance. In other words, rather than weighting actual rewards, we **estimate the expected reward of the target policy in each state** using a learned model, and then average those estimates. This approach is analogous to the **“G-method” or regression approach** in causal inference. For example, in a contextual bandit setting, one could train a predictor $\hat{r}(x,a)$ for reward given context $x$ and action $a$, and then compute the policy value as $V^{DM}(\pi) = \frac{1}{n}\sum\_{i=1}^n \sum\_{a \in A} \hat{r}(x\_i, a), \pi(a|x\_i)$. This formula says: for each logged context $x\_i$ in the data, predict the reward for each action $a$ using the model $\hat{r}$, then take the expectation under the target policy $\pi$ (i.e. weighted by $\pi(a|x\_i)$) and average over data points. In a full RL (sequential) scenario, the Direct Method typically means learning an approximate **$Q^\pi(s,a)$ or $V^\pi(s)$** for the target policy by **fitting to the offline data** (often using dynamic programming or regression), then using that to estimate performance.

In practice for episodic RL, one common approach is **Fitted Q Evaluation (FQE)**: start with a function approximator for $Q(s,a)$, and iteratively minimize the Bellman error for the target policy using the offline transitions. Specifically, we can train $\hat{Q}(s,a)$ on the dataset such that it satisfies $\hat{Q}(s,a) \approx r + \gamma \mathbb{E}*{a' \sim \pi}[ \hat{Q}(s', a') ]$ for each transition $(s,a,r,s')$ in the data. Once $\hat{Q}$ is learned, we estimate the value by $V(\pi) = \mathbb{E}*{s\_0 \sim \mathcal{D}}[ \mathbb{E}\_{a \sim \pi}[ \hat{Q}(s\_0,a) ] ]$ (i.e. average the estimated $V^\pi(s\_0)$ over initial states in the dataset). If the policy’s own value network is available (e.g. PPO’s critic), one could also use that as the direct estimate $V^\pi(s)$ for each state—this is effectively a form of direct method too (using a model trained during policy optimization). The strength of DM is that it uses potentially **all data** to learn a smoother value function, yielding **low variance** estimates. However, it is **biased** if the learned model $\hat{Q}$ is inaccurate or the function class is mis-specified. The bias can be hard to quantify because modeling error in complex environments is difficult to assess.

**Key points of DM:**

* *Model-based:* Requires learning a model of rewards and/or transitions, or directly the value function for the target policy using the behavior data.
* *Low variance:* Once the model is learned, the estimate $V^{DM}$ is a direct computation (no high-variance weighting of rare events). Thus DM tends to be more stable.
* *Potential bias:* If the model $\hat{Q}$ or $\hat{V}$ is wrong (due to function approximation error or insufficient data in some regions), the OPE estimate will be biased. Unlike IS, DM does **not** guarantee unbiasedness even with infinite data unless the model class can represent the true $Q^\pi$.
* *No explicit $\mu$ needed:* DM does not use behavior policy probabilities; it uses the transitions and rewards as data for regression. This can be advantageous if the logging policy probabilities are unknown or unreliable.
* *In our context:* We will implement a simplified FQE for LunarLander-v2. This will involve iterating over the dataset to improve $\hat{Q}$ estimates for the target PPO policy.

**Implementation:** Below is a from-scratch implementation of a simple **Fitted Q Evaluation** for the target policy. We represent $\hat{Q}(s,a)$ in a table/dictionary for states seen (note: LunarLander’s state is continuous, so in a real scenario we would use function approximation; here we discretize by using the raw state tuple as a key, which is only for didactic purposes). We iterate a fixed number of times over the dataset, updating $\hat{Q}(s,a)$ towards the one-step Bellman target $r + \gamma \sum\_{a'} \pi(a'|s') \hat{Q}(s',a')$. This uses the target policy $\pi$ (PPO) to weight the next-state actions. Finally, we estimate the policy’s value by averaging $\hat{V}(s\_0)$ for initial states in the episodes.

```python
# direct_method_ope.py
import json

def off_policy_eval_direct_method(episodes, eval_policy, gamma=1.0, iterations=100, alpha=0.1):
    """
    Off-Policy Evaluation using the Direct Method (Fitted Q Evaluation).
    - episodes: list of episodes (each episode is a list of step dicts with state, action, reward, next_state, done, etc.).
    - eval_policy: object with eval_policy.prob(state, action) to get target π(a|s).
    - gamma: discount factor (1.0 for undiscounted total reward, or <1 for discounted).
    - iterations: number of iterations for value function fitting.
    - alpha: learning rate for updates (simple constant-step TD updates).
    Returns: Estimated value of the target policy.
    """
    # Initialize Q-table for all state-action pairs seen in data
    Q = {}
    # Also track unique actions (assuming discrete action space)
    action_set = set()
    for ep in episodes:
        for step in ep:
            s = tuple(step["state"])        # convert state (list/array) to tuple for dictionary key
            a = step["action"]
            action_set.add(a)
            Q[(s, a)] = 0.0  # initialize Q estimate
    
    possible_actions = sorted(action_set)
    # Iteratively update Q estimates using the Bellman expectation equation for π
    for it in range(iterations):
        for ep in episodes:
            for step in ep:
                s = tuple(step["state"])
                a = step["action"]
                r = step["reward"]
                next_s = tuple(step["next_state"])
                done = step.get("done", False)
                # Compute target value
                if done:
                    # If episode ended, no future value
                    target = r
                else:
                    # Estimate V_pi(next_state) = E_{a' ~ π}[ Q(next_state, a') ]
                    V_next = 0.0
                    for a2 in possible_actions:
                        pi_prob = eval_policy.prob(next_s, a2)
                        # If Q for (next_s, a2) not seen, initialize it to 0
                        Q_next = Q.get((next_s, a2), 0.0)
                        V_next += pi_prob * Q_next
                    target = r + gamma * V_next
                # Update Q(s,a) towards the target (TD(0) style update)
                Q[(s, a)] = Q[(s, a)] + alpha * (target - Q[(s, a)])
    # After fitting, compute the average value over initial states of episodes
    total_init_value = 0.0
    for ep in episodes:
        if len(ep) == 0:
            continue
        s0 = tuple(ep[0]["state"])
        # Estimate V_pi(s0) = sum_a π(a|s0) * Q(s0, a)
        V0 = 0.0
        for a in possible_actions:
            pi_prob = eval_policy.prob(s0, a)
            V0 += pi_prob * Q.get((s0, a), 0.0)
        total_init_value += V0
    return total_init_value / len(episodes)

# Example usage:
# episodes_data = json.load(open("episodes.json"))
# value_estimate = off_policy_eval_direct_method(episodes_data, eval_policy, gamma=1.0)
# print("Estimated policy value (Direct Method):", value_estimate)
```

In this code, we use a simple tabular approach for clarity. Each iteration sweeps through the dataset and updates $\hat{Q}(s,a)$. This is essentially performing **off-policy policy evaluation** for the target policy on the fixed batch of data (similar to policy evaluation in dynamic programming, but using the sample transitions to approximate expectations). Over many iterations, $\hat{Q}$ should converge to an estimate of the true $Q^\pi$ on the support of the data. We then compute the initial state value by taking a π-weighted sum of $\hat{Q}(s\_0,a)$.

This direct method is a crude example (in practice one would use function approximation like neural networks for continuous state spaces, and more sophisticated training like batch TD or LSTD). Nonetheless, it illustrates the principle: **learn a value function from data, then compute the policy’s value from that**. The advantage is that we smooth over many samples when learning $\hat{Q}$, so the estimate might be more stable than high-variance importance sampling. The disadvantage is clear: if our learned $\hat{Q}$ is poor (due to limited data or function approximation error), the estimate could be biased arbitrarily.

## Doubly Robust (DR) Method

**Theory:** The Doubly Robust estimator combines the strengths of the direct method and importance sampling, providing an estimate that is **unbiased** as long as *either* the model is correct or the importance weighting is correct (hence “doubly robust”). It achieves this by using the model’s estimate as a baseline and only using importance sampling to correct the **residual errors** of the model’s predictions. The DR estimator was first introduced for contextual bandits and later extended to sequential decision problems. In a simplified bandit form, the doubly robust estimator for value can be written as:

$V^{DR}(\pi) \;=\; \frac{1}{n}\sum_{i=1}^n \Big[ \underbrace{(r_i - \hat{r}(x_i,a_i)) \frac{\pi(a_i|x_i)}{\mu(a_i|x_i)}}_{\text{IS correction term}} \;+\; \underbrace{\sum_{a \in \mathcal{A}} \hat{r}(x_i,a)\, \pi(a|x_i)}_{\text{Direct term}} \Big] \,.$

This formula shows two parts inside the sum: (1) an IS-weighted **error term** $(r\_i - \hat{r}(x\_i,a\_i))$ which adjusts for the difference between actual reward $r\_i$ and the model’s predicted reward $\hat{r}(x\_i,a\_i)$ for the taken action, and (2) a **direct term** which is just the model’s predicted value for context $x\_i$ under policy $\pi$ (like $V^{DM}$). If the model $\hat{r}$ is perfect, the correction term’s numerator $(r\_i - \hat{r}(x\_i,a\_i))$ is zero in expectation, so DR collapses to the low-variance direct method. If the model is poor, the IS term corrects the estimate, and as long as the behavior policy probabilities are correct (and we have sufficient data), the estimator remains unbiased. In short, DR gives us “two chances” for an accurate estimate: it will be accurate if either the model was good or the importance weights were accurate, and only fails if both are bad. Empirically, DR often has much lower variance than plain IS while avoiding the large bias of a broken direct method.

For sequential multi-step scenarios, the DR estimator can be implemented via a **stepwise recursive formula** (as derived by Jiang & Li, 2016). Essentially, one uses an approximate $Q$-function for the target policy (from the direct method) and then “corrects” each step’s value. One convenient form is to compute the DR estimate by **going backwards through each episode**. If $\hat{Q}(s,a)$ is our learned Q-function (our model from the direct method), and $\hat{V}(s) = \sum\_a \pi(a|s)\hat{Q}(s,a)$, then for a trajectory we can compute:

$V_{DR}^{(episode)} = \hat{V}(s_0) + \sum_{t=0}^{T-1} \rho_{0:t} \Big( r_t + \gamma \hat{V}(s_{t+1}) - \hat{Q}(s_t, a_t) \Big) \,,$

where $\rho\_{0\:t} = \prod\_{i=0}^t \frac{\pi(a\_i|s\_i)}{\mu(a\_i|s\_i)}$ is the importance weight up to time $t$. This equation might look complex, but it’s basically saying: start with the model’s prediction $\hat{V}(s\_0)$, and add corrections at each step *t* by comparing actual reward $r\_t$ plus expected future value $\hat{V}(s\_{t+1})$ to the model’s predicted $Q$ for the taken action. The correction is weighted by $\rho\_{0\:t}$ to account for off-policy sampling. If the model is exact, each correction term’s expectation is zero, so we just get the initial $\hat{V}(s\_0)$. If the model is wrong, the corrections adjust it using actual data. This estimator is proven to be unbiased and often has significantly reduced variance compared to ordinary IS.

**Key points of DR:**

* *Unbiased under weaker conditions:* DR only needs either the model or the behavior policy probabilities to be correct to achieve unbiasedness. In practice, behavior policy is known (data from a known logging policy) or model can be improved, so this gives robustness.
* *Variance reduction:* Uses the model as a control variate. If the model is reasonably accurate, the IS terms are small and variance is greatly reduced compared to pure IS. In fact, DR variance = variance of IS when model is zero (no baseline), and gets better as model accuracy improves.
* *Complexity:* Requires training a model (like in DM) **and** computing importance weights (like IS). So it combines the requirements of both. If either piece fails badly (model totally wrong *and* behavior probabilities inaccurate or extremely small for important actions), DR can still perform poorly. But usually at least one piece is somewhat reliable.
* *Use in practice:* DR is often the preferred OPE estimator when one has a decent model of the environment or value function. Modern RL libraries (like Ray RLlib) have implementations of DR OPE for off-policy evaluation.

**Implementation:** To implement DR, we first need a $\hat{Q}$ or $\hat{V}$ from the direct method. We can reuse the $Q$ learned in the code above (or we could train a separate model for Q specifically for DR – sometimes one uses half the data to fit the model and half to do the IS correction to avoid overfitting, but we won’t complicate that here). Given $\hat{Q}$ and $\hat{V}$, we’ll apply the sequential DR formula by iterating **backwards** through each episode. The code below assumes we call the direct method first to get a filled-in Q-table (or we could integrate it to compute on the fly). It then computes the DR estimate for each episode and averages them:

```python
# doubly_robust_ope.py
import json

def off_policy_eval_doubly_robust(episodes, eval_policy, Q, gamma=1.0):
    """
    Off-Policy Evaluation using the Doubly Robust estimator.
    - episodes: list of episodes (each a list of step dicts).
    - eval_policy: object with eval_policy.prob(state, action) for target policy probabilities.
    - Q: dict or function for Q_hat(s,a) = estimated Q-value of target policy (from direct method).
         This can be the Q table learned by off_policy_eval_direct_method.
    - gamma: discount factor.
    Returns: Estimated value of the target policy.
    """
    estimates = []
    for ep in episodes:
        # Compute doubly robust estimate for this episode
        # Start from V_hat(s_0) and then add corrections
        V_DR = 0.0
        # We'll accumulate from the final step backwards
        # Initialize cumulative importance weight
        cum_rho = 1.0
        # It's easier to loop forward computing cum_rho and then loop backward computing corrections,
        # but we can also compute backward recursively. Here, we'll do backward for direct implementation.
        # First, compute V_hat(s0) which is the baseline estimate for this episode:
        if len(ep) > 0:
            s0 = tuple(ep[0]["state"])
            # \hat{V}(s0) = sum_a π(a|s0) Q_hat(s0,a)
            V0_hat = 0.0
            # Determine possible actions from Q (keys for s0)
            actions_for_s0 = [a for (s,a) in Q.keys() if s == s0]
            if not actions_for_s0:
                # If s0 not in Q (not seen in training), we assume 0 value
                V0_hat = 0.0
            else:
                for a in actions_for_s0:
                    V0_hat += eval_policy.prob(s0, a) * Q.get((s0, a), 0.0)
            V_DR = V0_hat
        else:
            V_DR = 0.0
        # Now add step-by-step corrections
        # We iterate through each time step of the episode
        for t, step in enumerate(ep):
            s = tuple(step["state"]); a = step["action"]; r = step["reward"]
            next_s = tuple(step["next_state"]); done = step.get("done", False)
            # importance weight for this step
            if "behavior_policy_prob" in step:
                mu_prob = step["behavior_policy_prob"]
            else:
                mu_prob = step["behavior_policy_probs"][a]
            pi_prob = eval_policy.prob(s, a)
            if mu_prob == 0:
                cum_rho = 0.0
            else:
                cum_rho *= (pi_prob / mu_prob)
            # Compute the one-step DR correction term: ρ_{0:t} * [r_t + γ V_hat(s_{t+1}) - Q_hat(s_t, a_t)]
            # Calculate Q_hat(s_t, a_t) and V_hat(s_{t+1})
            Q_hat_sa = Q.get((s, a), 0.0)
            if done:
                # If episode terminates at this step, V_hat(next) = 0
                V_hat_next = 0.0
            else:
                # Compute \hat{V}(s_{t+1})
                V_hat_next = 0.0
                actions_for_next = [a2 for (s2,a2) in Q.keys() if s2 == next_s]
                if actions_for_next:
                    for a2 in actions_for_next:
                        V_hat_next += eval_policy.prob(next_s, a2) * Q.get((next_s, a2), 0.0)
            # Add the correction term to V_DR
            V_DR += cum_rho * (r + gamma * V_hat_next - Q_hat_sa)
        estimates.append(V_DR)
    # Return the average DR estimate over all episodes
    return sum(estimates) / len(estimates)

# Example usage:
# episodes_data = json.load(open("episodes.json"))
# # First, fit Q via direct method:
# Q_hat = {}  # suppose we filled this using off_policy_eval_direct_method (we can modify that function to return Q as well)
# value_estimate = off_policy_eval_doubly_robust(episodes_data, eval_policy, Q_hat, gamma=1.0)
# print("Estimated policy value (Doubly Robust):", value_estimate)
```

In this code, `Q` is expected to be the dictionary of Q-values learned for the target policy (for example, we could modify `off_policy_eval_direct_method` to return the Q table in addition to the value estimate, and pass that here). The DR estimator then for each episode: starts with the model’s estimate $V\_0 = \hat{V}(s\_0)$ (as `V_DR = V0_hat`), and then goes through each step to accumulate the correction terms. We maintain `cum_rho` as $\rho\_{0\:t}$ (the product of importance ratios up to the current step). For each step, we compute the bracket $[r\_t + \gamma \hat{V}(s\_{t+1}) - \hat{Q}(s\_t,a\_t)]$ and multiply by the cumulative weight `cum_rho`, adding this to `V_DR`. The result after the last step is the doubly robust estimate of that episode’s return under the target policy. Finally, we average these per-episode estimates.

This implementation directly reflects the mathematical form of the sequential DR estimator. If the $\hat{Q}$ values are good, the correction terms will be small (their expectation is zero if $\hat{Q} = Q^\pi$), so `V_DR` will be close to the direct estimate but with low variance. If $\hat{Q}$ is poor, the importance-weighted corrections adjust for it, falling back toward an IS-like estimate. In our code, if an action had zero probability under $\mu$ (coverage failure), we set `cum_rho = 0` from that point, so further corrections also zero out (this is a simple way to handle unseen actions: effectively trust the model thereafter, though in practice one might exclude those trajectories or impose a cap on weights).

**Conclusion:** We have presented implementations and explanations for three OPE methods:

* **Importance Sampling (IS):** reweights actual returns by probability ratios; unbiased but high variance.
* **Direct Method (DM):** learns a model (e.g. Q-function) for the target policy from data and estimates value from it; low variance but can be biased.
* **Doubly Robust (DR):** combines IS and DM by using the model as baseline and IS for error correction; it is unbiased under mild conditions and typically lower variance than IS.

These methods allow us to estimate how a PPO policy (our target) would perform in **LunarLander-v2** using episodes generated by another policy, without deploying the PPO policy in the environment. In practice, one should also compute confidence intervals for these estimates (e.g. via bootstrap or concentration bounds) to understand uncertainty. Nonetheless, OPE provides a powerful toolkit for policy evaluation in offline RL and counterfactual analysis.
