@dataclass class RLlibGreedyHead(BaseHead): """Adaptador mínimo para usar una policy de RLlib como evaluation_policy en SCOPE-RL (acción discreta).""" name: str = "rllib_greedy" rllib_policy: Any = None # instancia de RLlib Policy (p. ej. algo.get_policy()) action_size: int = None # n° de acciones discretas base_policy: Any = None action_type = 'discrete' # --- parche clave: evitar deepcopy de la policy --- def __deepcopy__(self, memo): # Creamos una nueva instancia copiando solo campos “seguros”, # y reusamos la MISMA referencia a rllib_policy. new = RLlibGreedyHead( name=self.name, rllib_policy=self.rllib_policy, # <- misma referencia, no deepcopy action_size=self.action_size, base_policy=self.rllib_policy, ) memo[id(self)] = new return new # --- opcional: hacer que el objeto sea “pickle-safe” si SCOPE-RL paraleliza --- def __getstate__(self): d = self.__dict__.copy() # No intentes picklear la policy de RLlib (no es necesaria para serializar el “head”) d["rllib_policy"] = None return d def __setstate__(self, state): self.__dict__.update(state) # El llamador debe volver a asignar la policy si hace falta tras un pickle/unpickle. # En la práctica, SCOPE-RL no debería necesitar picklear el head con la policy dentro. # --- Obligatorios para BaseHead en flujos discretos --- def calc_action_choice_probability(self, x: np.ndarray) -> np.ndarray: """Devuelve prob(a|s) con una distribución one-hot de la acción greedy (determinista).""" states_iter, n = _iter_states(x) probs = np.zeros((n, self.action_size), dtype=float) i = 0 for s in states_iter: a, _, _ = self.rllib_policy.compute_single_action(s, explore=False) probs[i, int(a)] = 1.0 i += 1 return probs def calc_pscore_given_action(self, x: np.ndarray, action: np.ndarray) -> np.ndarray: """Devuelve pscore de la acción observada bajo la policy (1 si coincide con la greedy, 0 en otro caso).""" states_iter, n = _iter_states(x) action = np.asarray(action).reshape(-1) p = np.zeros((n,), dtype=float) i = 0 for s in states_iter: a_star, _, _ = self.rllib_policy.compute_single_action(s, explore=False) p[i] = 1.0 if int(a_star) == int(action[i]) else 0.0 i += 1 return p def predict_online(self, x: np.ndarray) -> np.ndarray: """Acción greedy por estado (vector de shape (n,)).""" states_iter, n = _iter_states(x) acts = np.zeros((n,), dtype=int) i = 0 for s in states_iter: a, _, _ = self.rllib_policy.compute_single_action(s, explore=False) acts[i] = int(a) i += 1 return acts def sample_action_and_output_pscore(self, x: np.ndarray) -> Tuple[np.ndarray, np.ndarray]: """ Requerido por BaseHead. Devuelve (acciones, pscore_de_acciones) para los estados x, donde pscore es prob(a|s) bajo esta policy. Como es determinista (greedy), pscore = 1.0 para la acción devuelta. """ states_iter, n = _iter_states(x) acts = np.zeros((n,), dtype=int) pscore = np.ones((n,), dtype=float) # determinista i = 0 for s in states_iter: a, _, _ = self.rllib_policy.compute_single_action(s, explore=False) acts[i] = int(a) i += 1 return acts, pscore