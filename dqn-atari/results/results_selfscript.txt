loaded JSON: /opt/ml/code/episodes/120820251600/011125_generated_rllib_ppo_rllib_seed_0000_1000eps_300steps_exp_0
Transformed 1000 episodes with a total of 260658 steps
loaded JSON: /opt/ml/code/episodes/130820251600/011125_generated_rllib_ppo_rllib_seed_0000_1000eps_300steps_exp_0
Transformed 1000 episodes with a total of 212478 steps
Avg_Expecting_Return (BEH_POLICY) Value - RLLIB Generated episodes:  46.783 - STD  15.784
Avg_Expecting_Return (TARGET_POLICY) Value - RLLIB Generated episodes:  76.929 - STD  16.689


######################################################################

03.01_oppe_from_scratch_is

Avg_Expecting_Return (BEH_POLICY) Value - RLLIB Generated episodes:  46.783 - STD  15.784
Avg_Expecting_Return (TARGET_POLICY) Value - RLLIB Generated episodes:  76.929 - STD  16.689
Calculating IS, WIS from scratch
loaded JSON: /opt/ml/code/episodes/120820251600/011125_generated_rllib_ppo_rllib_seed_0000_2000eps_300steps_exp_0
Transformed 2000 episodes with a total of 521601 steps

Ordinary IS: 1.5115856051469481e-37
Ordinary IS (logprob): 9.711437639409819e-08
Ordinary IS (vect logprob): 9.711437639409819e-08
Weighted IS (logp): 47.116515400342564
Weighted IS (vect logp): 47.116515400342564

######################################################################

DM best model: [FQE] Epoch 100/200 avg_loss=11.635035

[FQE] Epoch 100/200 avg_loss=11.635035
[FQE] Saved checkpoint: /mnt/azureml/cr/j/b93fefc5cf43477e9b0b767153a92ff3/cap/data-capability/wd/model/fqe_checkpoints/fqe_epoch_105.pt
[FQE] Epoch 110/200 avg_loss=11.928526
[FQE] Saved checkpoint: /mnt/azureml/cr/j/b93fefc5cf43477e9b0b767153a92ff3/cap/data-capability/wd/model/fqe_checkpoints/fqe_epoch_120.pt
[FQE] Epoch 120/200 avg_loss=12.378288
[FQE] Epoch 130/200 avg_loss=13.029963
[FQE] Saved checkpoint: /mnt/azureml/cr/j/b93fefc5cf43477e9b0b767153a92ff3/cap/data-capability/wd/model/fqe_checkpoints/fqe_epoch_135.pt
[FQE] Epoch 140/200 avg_loss=13.995070
[FQE] Saved checkpoint: /mnt/azureml/cr/j/b93fefc5cf43477e9b0b767153a92ff3/cap/data-capability/wd/model/fqe_checkpoints/fqe_epoch_150.pt
[FQE] Epoch 150/200 avg_loss=15.320837
[FQE] Epoch 160/200 avg_loss=16.643313
[FQE] Saved checkpoint: /mnt/azureml/cr/j/b93fefc5cf43477e9b0b767153a92ff3/cap/data-capability/wd/model/fqe_checkpoints/fqe_epoch_165.pt
[FQE] Epoch 170/200 avg_loss=18.796383
[FQE] Saved checkpoint: /mnt/azureml/cr/j/b93fefc5cf43477e9b0b767153a92ff3/cap/data-capability/wd/model/fqe_checkpoints/fqe_epoch_180.pt
[FQE] Epoch 180/200 avg_loss=20.816773
[FQE] Epoch 190/200 avg_loss=22.679195
[FQE] Saved checkpoint: /mnt/azureml/cr/j/b93fefc5cf43477e9b0b767153a92ff3/cap/data-capability/wd/model/fqe_checkpoints/fqe_epoch_195.pt
[FQE] Saved checkpoint: /mnt/azureml/cr/j/b93fefc5cf43477e9b0b767153a92ff3/cap/data-capability/wd/model/fqe_checkpoints/fqe_epoch_200.pt
[FQE] Epoch 200/200 avg_loss=25.609360


########################################################################################3
03.02_oppe_from_scratch_dm_2.py

Checkpoint loaded
loaded JSON: /opt/ml/code/episodes/120820251600/011125_generated_rllib_ppo_rllib_seed_0000_1000eps_300steps_exp_0
Transformed 1000 episodes with a total of 260658 steps
loaded JSON: /opt/ml/code/episodes/130820251600/011125_generated_rllib_ppo_rllib_seed_0000_1000eps_300steps_exp_0
Transformed 1000 episodes with a total of 212478 steps
Avg_Expecting_Return (BEH_POLICY) Value - RLLIB Generated episodes:  46.783 - STD  15.784
Avg_Expecting_Return (TARGET_POLICY) Value - RLLIB Generated episodes:  76.929 - STD  16.689
loaded JSON: /opt/ml/code/episodes/120820251600/011125_generated_rllib_ppo_rllib_seed_0000_2000eps_300steps_exp_0
Transformed 2000 episodes with a total of 521601 steps
Extrayendo estados iniciales s0 de cada episodio...
Número de episodios: 2000
Cargando modelo FQE...
Se cargó el checkpoint desde ./20260104_fqe_checkpoints, entrenadas 105 épocas con Avg. Loss 12.047094437613415
Calculando estimador DM (política estocástica)...
[DM estocástico] J_hat ≈ -2080.4624
Calculando estimador DM (política greedy w.r.t Q)...
[DM greedy] J_hat ≈ -7.1618



---->

checkpoint = torch.load(FQE_CHECKPOINT_PATH + '/fqe_epoch_80.pt')
Se cargó el checkpoint desde ./fqe_checkpoints, entrenadas 80 épocas con Avg. Loss 2.8818980979652298
Calculando estimador DM (política estocástica)...
[DM estocástico] J_hat ≈ -3613.9351
Calculando estimador DM (política greedy w.r.t Q)...
[DM greedy] J_hat ≈ -5.2253


#############################################################################################################
03.02_oppe_from_scratch_dm.py

loaded JSON: /opt/ml/code/episodes/120820251600/011125_generated_rllib_ppo_rllib_seed_0000_1000eps_300steps_exp_0
Transformed 1000 episodes with a total of 260658 steps
loaded JSON: /opt/ml/code/episodes/130820251600/011125_generated_rllib_ppo_rllib_seed_0000_1000eps_300steps_exp_0
Transformed 1000 episodes with a total of 212478 steps
Avg_Expecting_Return (BEH_POLICY) Value - RLLIB Generated episodes:  46.783 - STD  15.784
Avg_Expecting_Return (TARGET_POLICY) Value - RLLIB Generated episodes:  76.929 - STD  16.689
loaded JSON: /opt/ml/code/episodes/120820251600/011125_generated_rllib_ppo_rllib_seed_0000_2000eps_300steps_exp_0
Transformed 2000 episodes with a total of 521601 steps

Se cargó el checkpoint desde ./fqe_checkpoints, entrenadas 80 épocas con Avg. Loss 2.8818980979652298

Valor esperado estimado DM de la política PPO (retorno promedio inicial): -3.118 - STD: 18.660
Valor esperado estimado DR de la política PPO (retorno promedio inicial): 47.117 - STD: 16.082

##################################################################################################################

03.03_oppe_rllib_selfcontent_script.py

loaded JSON: /opt/ml/code/episodes/120820251600/011125_generated_rllib_ppo_rllib_seed_0000_1000eps_300steps_exp_0
Transformed 1000 episodes with a total of 260658 steps
loaded JSON: /opt/ml/code/episodes/130820251600/011125_generated_rllib_ppo_rllib_seed_0000_1000eps_300steps_exp_0
Transformed 1000 episodes with a total of 212478 steps
Avg_Expecting_Return (BEH_POLICY) Value - RLLIB Generated episodes:  46.783 - STD  15.784
Avg_Expecting_Return (TARGET_POLICY) Value - RLLIB Generated episodes:  76.929 - STD  16.689
2026-01-08 08:56:43,769	WARNING json_reader.py:261 -- Treating input directory as glob patterns: ['/opt/ml/code/episodes/120820251600/011125_generated_rllib_ppo_rllib_seed_0000_2000eps_300steps_exp_0/*.json', '/opt/ml/code/episodes/120820251600/011125_generated_rllib_ppo_rllib_seed_0000_2000eps_300steps_exp_0/*.zip']
Ordinary Importance Sampling (IS RLLIB) Value: 1.9778902337205626
Direct Method (DM) with RLLIB V_ expected estimation -10.975613594055176 and STD 6.487840134380448
Double Roboust (DR) with RLLIB V_ expected estimation -8.622335703923106 and STD 16.12636908490694


##################################################################################################################
